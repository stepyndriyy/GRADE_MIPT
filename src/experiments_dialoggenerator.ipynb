{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beliakovsi/Documents/MIPT/grade/env3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BlenderbotSmallForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotSmallConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  My friends are cool but they eat too many carbs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beliakovsi/Documents/MIPT/grade/env3.9/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  what kind of carbs do they eat? i don't know much about carbs.\n",
      "Human:  I'm not sure\n",
      "{'input_ids': tensor([[  42,  643,   46, 1430,   45,   52, 1176,  146,  177,  753, 2430,    3,\n",
      "            3,    3,  330, 1360,    3,  330,    3,   44,  444,   10,  753, 2430,\n",
      "           59,   52, 1176,   20,   14,   67,    8,   30,   70,  165,   72,  753,\n",
      "          372,  330,    3,    3,  330, 1360,    3,  330, 1360,   14,    8,   58,\n",
      "           48,  225,    5]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n",
      "Bot:  they eat a lot of carbs. carbs are high in fat, protein, and carbohydrates.\n"
     ]
    }
   ],
   "source": [
    "mname = \"facebook/blenderbot_small-90M\"\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n",
    "model.max_new_tokens = 1000\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "REPLY = \"I'm not sure\"\n",
    "print(\"Human: \", REPLY)\n",
    "\n",
    "NEXT_UTTERANCE = (\n",
    "    \"My friends are cool but they eat too many carbs.</s> <s>what kind of carbs do they eat? \"\n",
    "    \"i don't know much about carbs</s> \"\n",
    "    \"<s> I'm not sure.\"\n",
    ")\n",
    "inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "next_reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "\n",
    "class dialogGenerator:\n",
    "    def __init__(self, mname=\"facebook/blenderbot-400M-distill\"):\n",
    "        self.mname = mname\n",
    "        #self.config = BlenderbotSmallConfig(max_new_tokens=1000)\n",
    "        self.model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "        self.model.max_new_tokens = 1000 # TODO\n",
    "        self.model.max_length = 1000\n",
    "        self.context = []\n",
    "        self.message_separator = \"</s> <s>\"\n",
    "        return\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.context = []\n",
    "        return\n",
    "    \n",
    "    def message(self, next_message=None):\n",
    "        if next_message is not None:\n",
    "            self.context.append(next_message)\n",
    "\n",
    "        previous_dialog = self.message_separator.join(self.context)\n",
    "\n",
    "        inputs = self.tokenizer([previous_dialog], return_tensors=\"pt\")\n",
    "        #print(inputs)\n",
    "        reply_ids = self.model.generate(max_length=1000, **inputs)\n",
    "        self.context.append(self.tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "        return self.context[-1]\n",
    "    \n",
    "    def print_dialog(self):\n",
    "        return self.message_separator.join(self.context)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator = dialogGenerator(\"facebook/blenderbot-400M-distill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "{'input_ids': tensor([[4424,    2]]), 'attention_mask': tensor([[1, 1]])}\n",
      " Hi! How are you? I just got back from walking my dog. Do you have any pets?\n",
      "I'm fine thanl you. Where are you from?\n",
      "{'input_ids': tensor([[4424,    2,  228,    1, 4424,    8,  855,  366,  304,   38,  281,  404,\n",
      "          660,  665,  482, 3568,  395, 1784,   21,  946,  304,  360,  463,  286,\n",
      "         1272,   38,    2,  228,    1,  281,  476, 1435,  546,   83,  304,   21,\n",
      "         2354,  366,  304,  482,   38,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      " I am from the united states. I have a dog and a cat. What about you?\n",
      "NICE!\n",
      "{'input_ids': tensor([[4424,    2,  228,    1, 4424,    8,  855,  366,  304,   38,  281,  404,\n",
      "          660,  665,  482, 3568,  395, 1784,   21,  946,  304,  360,  463,  286,\n",
      "         1272,   38,    2,  228,    1,  281,  476, 1435,  546,   83,  304,   21,\n",
      "         2354,  366,  304,  482,   38,    2,  228,    1,  281,  632,  482,  271,\n",
      "          608, 1353, 3263,   21,  281,  360,  265, 1784,  298,  265, 2382,   21,\n",
      "          714,  458,  304,   38,    2,  228,    1,  432, 4035,   44,    8,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      " What kind of dog do you have? Mine is a labrador retriever.\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "#print(dialog_generator.message(next_message))\n",
    "for i in range(N):\n",
    "    next_message = input()\n",
    "    print(next_message)\n",
    "    print(dialog_generator.message(next_message))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_generator.start_new_dialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog_generator.print_dialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some</s> <s>data</s> <s>lots</s> <s>of</s> <s>strings\n"
     ]
    }
   ],
   "source": [
    "data = [\"some\", \"data\", \"lots\", \"of\", \"strings\"]\n",
    "separator = \"</s> <s>\"\n",
    "\n",
    "\n",
    "print(separator.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure if you're being serious or not, but I'm not sure either.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure if you're being serious or not, but I'm not sure either.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure if you're being serious or not, but I'm not sure either.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure if you're being serious or not, but I'm not sure either.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure if you're being serious or not, but I'm not sure either.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    #print(chat_history_ids)\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beliakovsi/Documents/MIPT/grade/env3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dialogGeneratorM:\n",
    "    def __init__(self, mname=\"microsoft/DialoGPT-medium\"):\n",
    "        self.mname = mname\n",
    "    \n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(mname, padding_side='right')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(mname)\n",
    "    \n",
    "        self.chat_history_ids = None\n",
    "    \n",
    "        # self.message_separator = \"</s> <s>\"\n",
    "   \n",
    "        return\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.chat_history_ids = None\n",
    "        return\n",
    "    \n",
    "    def update_history(self, history: list[str]):\n",
    "        for message in history:\n",
    "            new_user_input_ids = self.tokenizer.encode(message + self.tokenizer.eos_token, return_tensors='pt')\n",
    "            self.chat_history_ids = torch.cat([self.chat_history_ids, new_user_input_ids], dim=-1) if self.chat_history_ids is not None else new_user_input_ids\n",
    "        print(self.chat_history_ids)\n",
    "        return \n",
    "    \n",
    "    def message(self, next_message=None):\n",
    "        if (next_message == None):\n",
    "            print(\"JOPA\")\n",
    "            return\n",
    "            \n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = self.tokenizer.encode(next_message + self.tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([self.chat_history_ids, new_user_input_ids], dim=-1) if self.chat_history_ids is not None else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        self.chat_history_ids = self.model.generate(bot_input_ids, max_length=1000, pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "        return self.tokenizer.decode(self.chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(self.tokenizer.decode(self.chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "\n",
    "    \n",
    "    def print_dialog(self):\n",
    "        return self.message_separator.join(self.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17250, 50256, 17250,     0,  1058,    35, 50256,  2437,   389,   345,\n",
      "            30, 50256,    40,  1101,   922,    11,   703,   389,   345,    30,\n",
      "         50256]])\n"
     ]
    }
   ],
   "source": [
    "dialog_generator_test = dialogGeneratorM()\n",
    "dialog_generator_test.update_history(['Hi', 'Hi! :D', 'How are you?', 'I\\'m good, how are you?'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi alice, i'm sam. i've always wanted to go to the beach. what about you?\n",
      "I'm a student.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n",
      "I'm a student too.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "I'm a student too.\n"
     ]
    }
   ],
   "source": [
    "dialog_generator = dialogGeneratorM()\n",
    "#dialog_generator.update_history(['Hi! my name Alice, tell me about yourself'])\n",
    "N = 3\n",
    "#print(dialog_generator.message(next_message))\n",
    "for i in range(N):\n",
    "    next_message = input()\n",
    "    print(next_message)\n",
    "    print(dialog_generator.message(next_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotsDialogGenerator:\n",
    "    def __init__(self, speaker_a, speaker_b):\n",
    "        self.speaker_a = speaker_a\n",
    "        self.speaker_b = speaker_b\n",
    "        \n",
    "        self.dialog = []\n",
    "\n",
    "        return\n",
    "    \n",
    "    def create_dialog(self, max_len=5, starting_message=\"Hi! my name Alice, tell me about yourself\") -> list[str]:\n",
    "        self.speaker_a.clear_history()\n",
    "        self.speaker_b.clear_history()\n",
    "        \n",
    "        self.speaker_b.update_history([starting_message])\n",
    "        next_message = starting_message\n",
    "        self.dialog.append(next_message)\n",
    "        \n",
    "        for dialog_len in range(max_len):\n",
    "            response = self.speaker_a.message(next_message)\n",
    "            self.dialog.append(response)\n",
    "\n",
    "            next_message = self.speaker_b.message(response)\n",
    "            self.dialog.append(next_message)\n",
    "            \n",
    "        return self.dialog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17250,     0,   616,  1438, 14862,    11,  1560,   502,   546,  3511,\n",
      "         50256]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi! my name Alice, tell me about yourself', \" Hi, my name is samantha. I'm a college student. How about you?\", \"Hi! I'm a college student too!\", \" Nice to meet you! What are you studying? I'm studying to be a nurse.\", \"I'm studying to be a nurse too!\", \" That's great! Do you have any hobbies? I like to knit and crochet.\", \"I do have a few hobbies, but I'm not really into them. I'm a bit of a nerd.\"]\n"
     ]
    }
   ],
   "source": [
    "dialog_bots = BotsDialogGenerator(dialogGenerator(), dialogGeneratorM())\n",
    "\n",
    "print(dialog_bots.create_dialog(max_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe27a7cf60b51836df6450553e684a65b1dc5d4521eb0fded9f6d53b72e958a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
